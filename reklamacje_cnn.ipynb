{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rozpoznawanie typów reklamacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SeparableConv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładowanie danych z pliku xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapath='/mnt/c/dev/reklamacje/'\n",
    "datafile='reklamacje_20181106_train.xlsx'\n",
    "dane_surowe=pd.read_excel(os.path.join(datapath,datafile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dane_surowe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_surowe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame()\n",
    "input_data[['content','category']]=dane_surowe[['tresc_zgl','typ_train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing data\n",
    "# remove duplicates\n",
    "input_data.drop_duplicates(inplace=True)\n",
    "# remove empty\n",
    "input_data=input_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "input_data[input_data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "załadowanie słowników tłumaczeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Słownik synonimów / podmian\n",
    "\n",
    "podmiany=pd.read_excel(os.path.join(datapath,'roboczy_slownik_synonimow.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing of content text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(raw_texts,replacements):\n",
    "    \"\"\"\n",
    "    texts: np.Series containing strings to be preprocessed\n",
    "    replacements: pairs of what convert to what\n",
    "    return np.Series with corrected texts\n",
    "    \"\"\"\n",
    "    resulttext=raw_texts.str.lower()\n",
    "    for [co,naco] in replacements.values:\n",
    "       resulttext=resulttext.str.replace(re.compile(str(co)),str(naco))\n",
    "    return resulttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_texts=preprocess_texts(input_data['content'],podmiany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(raw_labels,interesting_labels):\n",
    "    \"\"\"\n",
    "    raw_labels: np.Series with labels\n",
    "    interesting_labels: list of labels you are interested in\n",
    "    \n",
    "    returns np.Series with corrected labels\n",
    "    \"\"\"\n",
    "    other_label='OTHER'\n",
    "    result_labels=raw_labels\n",
    "    result_labels=result_labels.apply(lambda x: x if x in interesting_labels else other_label)\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_categories=['XDR','XOA','XRF','XSP']\n",
    "prep_labels=preprocess_labels(input_data['category'],interesting_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_labels(labels,dictionary=None):\n",
    "    if dictionary==None:\n",
    "        cat_labels, uniques = pd.factorize(labels)\n",
    "    else:\n",
    "        None\n",
    "    return cat_labels, uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels,label_dict=categorize_labels(prep_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(cat_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to the disk\n",
    "pd.DataFrame(label_dict).to_excel(os.path.join(datapath,'slownik_kategorii.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data to train and validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train,texts_val,y_train,y_val=train_test_split(prep_texts.values,\n",
    "                                                     cat_labels,test_size=0.25,random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_val).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(texts_train.shape)\n",
    "print(y_train.shape)\n",
    "print(texts_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_words_per_sample(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_length_distribution(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 10000\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K,filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~')\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    print(f'Number of words in word_index={len(tokenizer.word_index)}')\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic=time.time()\n",
    "print('vectorizing texts...')\n",
    "# Vectorize texts.\n",
    "x_train, x_val, word_index = sequence_vectorize(texts_train, texts_val)\n",
    "toc=time.time()\n",
    "print('time:',toc-tic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance classes in training set\n",
    "\n",
    "# oversampling to boost minority classes\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "x_train_balanced, y_train_balanced = ros.fit_resample(x_train,y_train)\n",
    "x_val_balanced, y_val_balanced = ros.fit_resample(x_val,y_val)\n",
    "\n",
    "# shuffle to be sure \n",
    "x_train_balanced, y_train_balanced = shuffle(x_train_balanced, y_train_balanced, random_state=0)\n",
    "#x_train_balanced = x_train_balanced.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts_train.shape)\n",
    "print(y_train.shape)\n",
    "print(texts_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_train_balanced.shape)\n",
    "print(y_train_balanced.shape)\n",
    "print(x_val_balanced.shape)\n",
    "print(y_val_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pre-trained embedding file and get word to word vector mappings.\n",
    "embedding_matrix_all = {}\n",
    "# We are using fasttext generated embeddings.\n",
    "fname = os.path.join(datapath, 'emb200ft.vec')\n",
    "with open(fname) as f:\n",
    "    # get first line with emb size\n",
    "    firstline = f.readline()\n",
    "    #embedding size is a second number in the firs line of a file\n",
    "    embedding_dim=int(firstline.split(' ')[1])\n",
    "    print (f'embedding_dim={embedding_dim}')\n",
    "    for line in f:  # Every line contains word followed by the vector value\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_matrix_all[word] = coefs\n",
    "\n",
    "# Prepare embedding matrix with just the words in our word_index dictionary\n",
    "num_words = min(len(word_index) + 1, TOP_K)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOP_K:\n",
    "        continue\n",
    "    embedding_vector = embedding_matrix_all.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# save dictionary to disk\n",
    "pd.DataFrame(list(word_index.items())).to_excel(os.path.join(datapath,'slownik_word_index.xlsx'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)\n",
    "#print(embedding_matrix[1])\n",
    "print(len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for i in range(blocks):\n",
    "        model.add(Conv1D(filters=(i+1)*filters, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit on the number of features. We use the top 20K features.\n",
    "# TOP_K = 20000\n",
    "\n",
    "\n",
    "def train_sequence_model(data,\n",
    "                         num_classes,\n",
    "                         learning_rate=1e-3,\n",
    "                         epochs=1000,\n",
    "                         batch_size=128,\n",
    "                         blocks=2,\n",
    "                         filters=64,\n",
    "                         dropout_rate=0.2,\n",
    "                         embedding_dim=200,\n",
    "                         kernel_size=3,\n",
    "                         pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (x_train, y_train), (x_val, y_val), word_index = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    tic=time.time()\n",
    "    print('creating model...')\n",
    "    \n",
    "    unexpected_labels = [v for v in y_val if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=False,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    # callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            # callbacks=callbacks,\n",
    "            validation_data=(x_val, y_val),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # Save model.\n",
    "    model.save('reklamacje_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata=((x_train_balanced, y_train_balanced), (x_val_balanced, y_val_balanced), word_index)\n",
    "\n",
    "myaccuracy, myloss, mymodel = train_sequence_model(mydata,\n",
    "                     num_classes=len(label_dict),\n",
    "                     learning_rate=1e-2,\n",
    "                     epochs=7,\n",
    "                     batch_size=128,\n",
    "                     blocks=2,\n",
    "                     filters=16,\n",
    "                     dropout_rate=0.1,\n",
    "                     embedding_dim=embedding_dim,\n",
    "                     kernel_size=3,\n",
    "                     pool_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate statistics per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = mymodel.predict(x_val) \n",
    "y_preds = y_prob.argmax(axis=-1)\n",
    "cm = metrics.confusion_matrix(y_val, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(list(y_val),list(y_preds),labels=[1,2,3,4],target_names=label_dict[1:]))\n",
    "#print(metrics.classification_report(list(y_val),list(y_preds),target_names=label_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_val).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mymodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = mymodel.predict(x_train) \n",
    "y_preds = y_prob.argmax(axis=-1)\n",
    "cm_train = metrics.confusion_matrix(y_train, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_train = pd.DataFrame(cm_train, label_dict, label_dict)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.0)#for label size\n",
    "sn.heatmap(df_cm_train, annot=True, annot_kws={\"size\": 12})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(list(y_train),list(y_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_ngram_model(data):\n",
    "    \"\"\"Tunes n-gram model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select parameter values to try.\n",
    "    num_layers = [1, 2, 3]\n",
    "    num_units = [4, 6, 10]\n",
    "    #dropouts =[0.3,0.4,0.5]\n",
    "\n",
    "    # Save parameter combination and results.\n",
    "    params = {\n",
    "        'layers': [],\n",
    "        'units': [],\n",
    "        'accuracy': [],\n",
    "        'loss':[],\n",
    "        'f1':[],\n",
    "    }\n",
    "    \n",
    "    (x_train, y_train), (x_val, y_val) = data\n",
    "\n",
    "    # Iterate over all parameter combinations.\n",
    "    for layers in num_layers:\n",
    "        for units in num_units:\n",
    "                params['layers'].append(layers)\n",
    "                params['units'].append(units)\n",
    "                print(f'parameters: layers-{layers}, units-{units}')\n",
    "                myaccuracy, myloss, mymodel = train_ngram_model(data,\n",
    "                      num_classes=len(label_dict),\n",
    "                      learning_rate=4e-3,\n",
    "                      epochs=7,\n",
    "                      batch_size=128,\n",
    "                      layers=layers,\n",
    "                      units=units,\n",
    "                      dropout_rate=0.4,\n",
    "                      l2=0.005)\n",
    "                y_prob = mymodel.predict(x_val) \n",
    "                y_preds = y_prob.argmax(axis=-1)\n",
    "                myf1=metrics.f1_score(list(y_val),list(y_preds),labels=[1,2,3,4])\n",
    "                print((f'Accuracy: {myaccuracy}, Loss: {myloss}, F1: {myf1}, Parameters: (layers={layers}, units={units})'))\n",
    "                params['accuracy'].append(myaccuracy)\n",
    "                params['loss'].append(myloss)\n",
    "                params['f1'].append(myf1)\n",
    "    #_plot_parameters(params)\n",
    "    return params\n",
    "    \n",
    "def _plot_parameters(params):\n",
    "    \"\"\"Creates a 3D surface plot of given parameters.\n",
    "\n",
    "    # Arguments\n",
    "        params: dict, contains layers, units and accuracy value combinations.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_trisurf(params['layers'],\n",
    "                    params['units'],\n",
    "                    params['accuracy'],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    antialiased=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wyniki = tune_ngram_model(mydata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
